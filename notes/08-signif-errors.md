# [fit] Significance 
# and
# [fit] Errors

---

# Recap: Statistical significance

A p-value of 0.05 or below is conventionally called “statistically significant” <br>

A p-value of 0.01 or below is conventionally called “highly statistically significant” <br>

CAUTION: These thresholds are arbitrary

---

# Significance levels

The threshold below which the p-value is deemed small enough to reject the null hypothesis

<br>

If p-value < $$\alpha$$, then reject H<sub>0</sub> 
<br>

If p-value ≥ $$\alpha$$, then do not reject H<sub>0</sub> 

---

# If the p-value is small...

Reject H<sub>0</sub> 
<br>

The sample would be extreme if H<sub>0</sub> were true
<br>

The results are statistically significant
<br>

We have evidence for H<sub>a</sub>

---

# If the p-value is **not** small...

**Do not reject** H<sub>0</sub>
<br>

The sample would not be too extreme if H<sub>0</sub> were true
<br>

The results are not statistically significant
<br>

The test is inconclusive: either H<sub>0</sub> or H<sub>a</sub> may be true

---

# Why can't we accept H<sub>a</sub>?

> For the logical fallacy of believing that a hypothesis has been proved to be true, merely because it is not contradicted by the available facts, has no more right to insinuate itself in statistical than in other kinds of scientific reasoning...
--R.A. Fisher

---

# [fit] Statistical
# [fit] Errors

---

### Two types of errors

---

# Probability of a Type I error

![inline](08-figs/rand_dist_hist1.pdf)

---

# Probability of a Type I error

![inline](08-figs/rand_dist_hist2.pdf)

---

# Probability of a Type I error

![inline](08-figs/rand_dist_hist3.pdf)

---

# How to set the signficance level

Consider which type of error is worse
<br>

If a Type I error is much worse, then set $$\alpha$$ lower
<br>

If a Type II error is much worse, then set $$\alpha$$ higher

---

> CAUTION: Statistically significant results are not required to be practically significant
